---
title: 分布式锁面试
date: 2018-1-2 20:09:04
tags: [zookeeper]
---

# 分布式锁要求



![image-20211020014047997](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211020014047997.png)

- 互斥性
  - 任意时间，只有一个客户端持有锁
- 无死锁
  - 计时 有所的客户端本科或者意外事件，锁仍然可以被获取
- 容错/高可用
  - 以redis为例，只要大部分的Redis的节点都活着，客户端就可以获取或者释放锁

![image-20211020015725911](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211020015725911.png)

### 分布式锁的实现思路

- 1）获取锁 ，进行业务操作
- 2）库存不足 释放锁
- 3）异常，释放锁
- 4）执行成功，释放锁


### 使用zookeeper的临时节点



## Java zookeeper分布式锁实现
``` java

import lombok.extern.slf4j.Slf4j;
import org.apache.curator.framework.CuratorFramework;
import org.apache.curator.framework.CuratorFrameworkFactory;
import org.apache.curator.framework.recipes.cache.PathChildrenCache;
import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;
import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;
import org.apache.curator.retry.ExponentialBackoffRetry;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.ZooDefs;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.util.concurrent.CountDownLatch;

/**
 * 思路如下： 首先创建一个 CountDownLatch  和 zkclient
 *
 * 如果 getLock 思路：如果说 锁路径已经存在，代表已经被锁了  获取锁失败就尝试等待， 通过 countdownlatch.await() 阻塞 等待被唤醒 获取锁
 *                  zookeeper会监测文件下路径的变化，如果锁路径被删除了 就代表锁可以重新获取了， 就通过countdownlatch.countDown来唤醒他
 *
 * @author guxiang (guxiang@rd.netease.com)
 * @date 2019/11/3 14:48
 */
@Configuration
@Slf4j
public class ZookeeperConfig {

    private CuratorFramework client = null;

    public static final String ZK_LOCK = "pk-zk-locks";

    public static final String DISTRIBUTED_LOCK = "pk-distributed-lock";

    public static CountDownLatch countDownLatch = new CountDownLatch(1);

    ZookeeperConfig(){
        client = CuratorFrameworkFactory.builder()
                .connectString("192.168.25.101:2181")
                .sessionTimeoutMs(10000)
                .retryPolicy(
                        new ExponentialBackoffRetry(1000, 3))
                .namespace("zk-namespace").build();
        client.start();
    }

    @Bean
    public CuratorFramework getClient(){
       client = client.usingNamespace("zk-namespace");
        try {
            if (client.checkExists().forPath("/"+ZK_LOCK)==null){
                client.create()
                        .creatingParentsIfNeeded()
                        .withMode(CreateMode.PERSISTENT)
                        .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE);
            }

            addWatch("/"+ZK_LOCK);
        } catch (Exception e) {
            e.printStackTrace();
        }

        return client;
    }

    public void addWatch(String path) throws Exception {
        PathChildrenCache cache = new PathChildrenCache(client, path, true);
        cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT);
        cache.getListenable().addListener(new PathChildrenCacheListener() {
            @Override
            public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent event) throws Exception {
                if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED)){
                    String path1 = event.getData().getPath();
                    if (path.contains(DISTRIBUTED_LOCK)){
                        countDownLatch.countDown();
                    }
                }
            }
        });
    }

    public void getLock(){
        try {
            client.create().creatingParentsIfNeeded()
                    .withMode(CreateMode.EPHEMERAL)
                    .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE)
                    .forPath("/"+ZK_LOCK+"/"+DISTRIBUTED_LOCK);
        } catch (Exception e) {
            e.printStackTrace();
            log.info("等待获取锁");
            if (countDownLatch.getCount()<=0){
                countDownLatch = new CountDownLatch(1);
            }

            try {
                countDownLatch.await();
            } catch (InterruptedException ex) {
                ex.printStackTrace();
            }
        }
        log.info("分布式锁获取成功");
    }


    public boolean releaseLock(){
        try {
            if (client.checkExists().forPath("/"+ZK_LOCK+"/"+DISTRIBUTED_LOCK) !=null){
                client.delete().forPath("/"+ZK_LOCK+"/"+DISTRIBUTED_LOCK);
            }
            log.info("分布式锁释放成功");
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }
}

```

![image-20211018175921853](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211018175921853.png)







![image-20211018180832889](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211018180832889.png)





### redis 





![image-20211018183003855](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211018183003855.png)



```
set  zhoujielun:1    随机值   1  ex  100  nx
```









#### 最初版redis分布式锁

![image-20211020015214978](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211020015214978.png)

上图的流程中存在redis加锁成功， 但是  expire 指令设置过期时间的失败

于是我们改用了

```
set  zhoujielun:1    uuid或threadid   ex  100  nx 

##  设置锁 数据为一个随机值    过期时间是100s    nx代表是互斥的  not exist 
```







### 使用lua脚本进行删除与使用lua脚本进行续命并且有续命次数限制

假设出现这种情况：  我线程A使用了50s   并且  加锁的过期时间是30s



我在 30s这个时刻， 有一个B线程来了，B线程的如果直接删除锁，那么就会出现B线程删除成功，但是A线程



![image-20211020190001395](https://gitee.com/guxiangfly/blogimage/raw/master/img/image-20211020190001395.png)









#### redis的同步

